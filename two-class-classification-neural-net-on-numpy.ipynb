{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "202f6df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from cv2 import cv2\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f63dda7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(a):\n",
    "    z = np.zeros_like(a)\n",
    "    return np.maximum(a, z)\n",
    "\n",
    "def diff_relu(a):\n",
    "    z = np.zeros_like(a)\n",
    "    z[a > 0] = 1\n",
    "    return z\n",
    "\n",
    "def softmax(Z):\n",
    "    Z = np.array(Z, dtype=np.float128)\n",
    "    return np.exp(Z) / np.sum(np.exp(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29dced0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_point(model, ep, error):\n",
    "    f = h5py.File('mlp_model_3_for_xray_ds_learned.hdf5', 'a')\n",
    "    grp = f.create_group(f\"epoch_{ep}\")\n",
    "    for i in range(len(model)):\n",
    "        grp_l = grp.create_group(f\"layer_{i}\")\n",
    "        dset = grp_l.create_dataset(\"weight\", data=model[i][0])\n",
    "        dset = grp_l.create_dataset(\"bias\", data=model[i][1])\n",
    "    dse = grp.create_dataset('error', data=error)\n",
    "    f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3873dc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_point_return(model, ep):\n",
    "    f = h5py.File('mlp_model_for_xray_ds_learned.hdf5', 'r')\n",
    "    epoch_grp = f[f\"epoch_{ep}\"]\n",
    "    for i in range(len(model)):\n",
    "        layer_grp = epoch_grp[f\"layer_{i}\"]\n",
    "        model[i][0] = layer_grp['weight']\n",
    "        model[i][1] = layer_grp['bias']\n",
    "    error = epoch_grp['error']\n",
    "    f.close\n",
    "    return model, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2784fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['PNEUMONIA', 'NORMAL']\n",
    "img_size = 150\n",
    "\n",
    "def get_training_data(data_dir):\n",
    "    data = [] \n",
    "    for label in labels: \n",
    "        path = os.path.join(data_dir, label)\n",
    "        class_num = labels.index(label)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n",
    "                data.append([resized_arr, class_num])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    np.random.shuffle(data)\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7b03e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV(4.5.5) /io/opencv/modules/imgproc/src/resize.cpp:4052: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
      "\n",
      "OpenCV(4.5.5) /io/opencv/modules/imgproc/src/resize.cpp:4052: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1987398/2387303108.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV(4.5.5) /io/opencv/modules/imgproc/src/resize.cpp:4052: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
      "\n",
      "OpenCV(4.5.5) /io/opencv/modules/imgproc/src/resize.cpp:4052: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = get_training_data('/home/ilnar/Documents/Jupyter-notebooks/datasets/chest_xray/chest_xray/train')\n",
    "test_data = get_training_data('/home/ilnar/Documents/Jupyter-notebooks/datasets/chest_xray/chest_xray/test')\n",
    "val_data = get_training_data('/home/ilnar/Documents/Jupyter-notebooks/datasets/chest_xray/chest_xray/val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "999905eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_data[:, 0], train_data[:, 1]\n",
    "X_test, y_test = test_data[:, 0], test_data[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23f20e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train)):\n",
    "    X_train[i] = X_train[i].reshape(150*150,)\n",
    "    \n",
    "for i in range(len(X_test)):\n",
    "    X_test[i] = X_test[i].reshape(150*150,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6971d3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5216,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a5d5f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train / 255.0, X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53d080ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(input_layer_count, output_layer_count):\n",
    "    W = np.random.normal(0, 2/100, (output_layer_count, input_layer_count))\n",
    "    b = np.ones((output_layer_count))\n",
    "    print(W.shape)\n",
    "    return [W, b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e260e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(model, z):\n",
    "    zs = [z]\n",
    "    activations = []\n",
    "    for i in range(len(model)):\n",
    "        W, b = model[i]\n",
    "        a = np.dot(W, zs[i]) + b\n",
    "        z = relu(a)\n",
    "\n",
    "        zs.append(z)\n",
    "        activations.append(a)\n",
    "    return activations, zs\n",
    "\n",
    "def backpropagation(model, activations, zs, X_, grad_zlE, lr):\n",
    "    for i in range(len(model) - 1, -1, -1):\n",
    "        if i == 0:\n",
    "            [W, b], a, z = model[i].copy(), activations[i], zs[i + 1]\n",
    "            grad_aE = diff_relu(a) * grad_zlE\n",
    "            grad_bE = grad_aE\n",
    "            b = b - lr * grad_bE\n",
    "            grad_w0E = grad_aE[np.newaxis].T * X_[np.newaxis]\n",
    "            W = W - lr * grad_w0E\n",
    "            model[i] = [W, b]\n",
    "            break\n",
    "        [W, b], a, z = model[i].copy(), activations[i], zs[i + 1]\n",
    "        z_l_1 = zs[i]\n",
    "\n",
    "        grad_aE = diff_relu(a) * grad_zlE\n",
    "\n",
    "        grad_bE = grad_aE\n",
    "        b = b - lr * grad_bE\n",
    "\n",
    "        grad_wE = np.dot(grad_aE[np.newaxis].T, z_l_1[np.newaxis])\n",
    "        \n",
    "        W = W - lr * grad_wE\n",
    "\n",
    "        grad_z_l_1E = np.dot(W.T, grad_aE)\n",
    "        grad_zlE = grad_z_l_1E\n",
    "\n",
    "        model[i] = [W, b]\n",
    "    return model\n",
    "\n",
    "def train(model, X_train, y_train, learning_rate=0.00001, epoch=1, record=False):\n",
    "    step = 20\n",
    "    lr = learning_rate\n",
    "    E_xs = np.array([])\n",
    "    try:\n",
    "        for ep in range(epoch):\n",
    "            print(\"epoch:\", ep)\n",
    "            index = 0\n",
    "            E_xi = 0\n",
    "            for l in range(len(X_train)):\n",
    "                X_ = X_train[l]\n",
    "                y_ = y_train[l]\n",
    "                z = X_\n",
    "\n",
    "                activations, zs = feed_forward(model, z)\n",
    "\n",
    "                y = softmax(zs[-1])\n",
    "\n",
    "                t = [1 if i == y_ else 0 for i in [0, 1]]\n",
    "                E_xi = (- np.dot(t, np.log(y)))\n",
    "                E_xs = np.append(E_xs, E_xi)\n",
    "\n",
    "                #if y_ == 1:\n",
    "                #    print(f\"{index} iteration:\", y_, \":\", y)\n",
    "                if index % step == 0:\n",
    "                    print(f\"{index} iteration: y = {y_}, predict {y}, loss: {E_xi}\")\n",
    "                    #print(f\"{index} iteration: y={y_}, predict {y}, Max loss in ({index - step}, {index}): {E_xs[-step:].max()}\")\n",
    "\n",
    "                i = t.index(1)\n",
    "\n",
    "                grad_yE = np.zeros_like(y)\n",
    "                grad_yE[i] = -1 / y[i]\n",
    "\n",
    "                grad_zlE = (y - t)\n",
    "\n",
    "                model = backpropagation(model, activations, zs, X_, grad_zlE, lr)\n",
    "\n",
    "                index += 1\n",
    "                #if index == early_stop:\n",
    "                #    return model, E_xs\n",
    "            check_point(model, ep, E_xs)\n",
    "    except KeyboardInterrupt:\n",
    "        if record:\n",
    "            check_point(model, f'eary_stopped', E_xs)\n",
    "        print(\"Training is early stopped!\")\n",
    "        return model, E_xs\n",
    "    return model, E_xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9cbf0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 22500)\n",
      "(50, 100)\n",
      "(2, 50)\n"
     ]
    }
   ],
   "source": [
    "model = [\n",
    "    layer(img_size**2, 100),\n",
    "    layer(100, 50),\n",
    "    layer(50, 2)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c01849ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e10443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1, er = check_point_return(model_1, 'eary_stopped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b90db4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "0 iteration: y = 0, predict [0.97260494 0.02739506], loss: 0.027777299514945973\n",
      "20 iteration: y = 0, predict [0.78927956 0.21072044], loss: 0.2366347000403576\n",
      "40 iteration: y = 1, predict [0.03094972 0.96905028], loss: 0.03143878333266517\n",
      "60 iteration: y = 0, predict [0.99520509 0.00479491], loss: 0.004806440627444602\n",
      "80 iteration: y = 0, predict [0.99165155 0.00834845], loss: 0.008383489977650814\n",
      "100 iteration: y = 0, predict [0.97545238 0.02454762], loss: 0.024853938068210145\n",
      "120 iteration: y = 0, predict [0.96972841 0.03027159], loss: 0.030739240933717348\n",
      "140 iteration: y = 0, predict [0.57029901 0.42970099], loss: 0.5615944840510996\n",
      "160 iteration: y = 0, predict [0.94356648 0.05643352], loss: 0.05808845568659124\n",
      "180 iteration: y = 0, predict [0.69779084 0.30220916], loss: 0.35983587639091713\n",
      "200 iteration: y = 1, predict [0.65675985 0.34324015], loss: 1.0693249226947485\n",
      "220 iteration: y = 0, predict [0.96904362 0.03095638], loss: 0.031445656475335135\n",
      "240 iteration: y = 0, predict [0.959723 0.040277], loss: 0.04111058154891677\n",
      "260 iteration: y = 1, predict [0.43196951 0.56803049], loss: 0.5655801815139739\n",
      "280 iteration: y = 0, predict [0.82522771 0.17477229], loss: 0.19209592391520372\n",
      "300 iteration: y = 1, predict [0.24507185 0.75492815], loss: 0.2811327059573356\n",
      "320 iteration: y = 1, predict [0.24502278 0.75497722], loss: 0.2810677052645321\n",
      "340 iteration: y = 0, predict [0.97793954 0.02206046], loss: 0.02230742992851161\n",
      "360 iteration: y = 0, predict [0.94978924 0.05021076], loss: 0.05151517169471582\n",
      "380 iteration: y = 1, predict [0.16137517 0.83862483], loss: 0.17599183152279588\n",
      "400 iteration: y = 1, predict [0.30917802 0.69082198], loss: 0.36987311534759637\n",
      "420 iteration: y = 1, predict [0.23774403 0.76225597], loss: 0.271472867383206\n",
      "440 iteration: y = 0, predict [0.39510418 0.60489582], loss: 0.9286058015352595\n",
      "460 iteration: y = 0, predict [0.9941511 0.0058489], loss: 0.005866074320372931\n",
      "480 iteration: y = 1, predict [0.05548756 0.94451244], loss: 0.057086423884387005\n",
      "500 iteration: y = 1, predict [0.273275 0.726725], loss: 0.319207137731354\n",
      "520 iteration: y = 1, predict [0.54457129 0.45542871], loss: 0.7865160812155673\n",
      "540 iteration: y = 0, predict [0.95976979 0.04023021], loss: 0.041061830141222444\n",
      "560 iteration: y = 0, predict [0.99012623 0.00987377], loss: 0.009922843769467499\n",
      "580 iteration: y = 0, predict [0.94135427 0.05864573], loss: 0.06043572515767556\n",
      "600 iteration: y = 1, predict [0.53394785 0.46605215], loss: 0.7634577313605824\n",
      "620 iteration: y = 0, predict [0.95781739 0.04218261], loss: 0.04309813653233784\n",
      "640 iteration: y = 0, predict [0.78217459 0.21782541], loss: 0.24567730039916658\n",
      "660 iteration: y = 0, predict [0.58376565 0.41623435], loss: 0.5382556627271903\n",
      "680 iteration: y = 0, predict [0.96949603 0.03050397], loss: 0.030978897312176474\n",
      "700 iteration: y = 0, predict [0.67678176 0.32321824], loss: 0.3904064242893151\n",
      "720 iteration: y = 1, predict [0.27083062 0.72916938], loss: 0.31584923323881564\n",
      "740 iteration: y = 0, predict [0.90913801 0.09086199], loss: 0.09525836745144205\n",
      "760 iteration: y = 0, predict [0.97205069 0.02794931], loss: 0.0283473290162177\n",
      "780 iteration: y = 0, predict [0.96636521 0.03363479], loss: 0.03421345388002515\n",
      "800 iteration: y = 0, predict [0.97185649 0.02814351], loss: 0.028547131980586463\n",
      "820 iteration: y = 0, predict [0.97951674 0.02048326], loss: 0.020695947437727468\n",
      "840 iteration: y = 1, predict [0.0144611 0.9855389], loss: 0.014566681943597557\n",
      "860 iteration: y = 0, predict [0.98750494 0.01249506], loss: 0.012573779428675134\n",
      "880 iteration: y = 0, predict [0.95831722 0.04168278], loss: 0.04257642370127834\n",
      "900 iteration: y = 0, predict [0.95408888 0.04591112], loss: 0.04699844136638935\n",
      "920 iteration: y = 0, predict [0.9856228 0.0143772], loss: 0.014481548505716507\n",
      "940 iteration: y = 0, predict [0.50416556 0.49583444], loss: 0.6848505728361413\n",
      "960 iteration: y = 0, predict [0.98723958 0.01276042], loss: 0.012842536167186988\n",
      "980 iteration: y = 1, predict [0.06954674 0.93045326], loss: 0.07208343737290396\n",
      "1000 iteration: y = 0, predict [0.99129332 0.00870668], loss: 0.008744801957003254\n",
      "1020 iteration: y = 0, predict [0.96768059 0.03231941], loss: 0.03285321999650971\n",
      "1040 iteration: y = 0, predict [0.83258898 0.16741102], loss: 0.18321517690626984\n",
      "1060 iteration: y = 1, predict [0.01776308 0.98223692], loss: 0.017922738152324848\n",
      "1080 iteration: y = 0, predict [0.97256299 0.02743701], loss: 0.02782043082406495\n",
      "1100 iteration: y = 1, predict [0.04840947 0.95159053], loss: 0.04962044820932122\n",
      "1120 iteration: y = 0, predict [0.98536021 0.01463979], loss: 0.014748007519748038\n",
      "1140 iteration: y = 0, predict [0.99064467 0.00935533], loss: 0.009399365161373146\n",
      "1160 iteration: y = 0, predict [0.95623082 0.04376918], loss: 0.04475595149714538\n",
      "1180 iteration: y = 1, predict [0.03339494 0.96660506], loss: 0.033965282898417465\n",
      "1200 iteration: y = 0, predict [0.9911025 0.0088975], loss: 0.008937316212278447\n",
      "1220 iteration: y = 0, predict [0.97397771 0.02602229], loss: 0.026366864077363005\n",
      "1240 iteration: y = 1, predict [0.972743 0.027257], loss: 3.6024449545976935\n",
      "1260 iteration: y = 0, predict [0.97709547 0.02290453], loss: 0.023170911490023652\n",
      "1280 iteration: y = 0, predict [0.98161026 0.01838974], loss: 0.018560934394758893\n",
      "1300 iteration: y = 0, predict [0.96530128 0.03469872], loss: 0.03531502029489303\n",
      "1320 iteration: y = 0, predict [0.84046415 0.15953585], loss: 0.1738009849757386\n",
      "1340 iteration: y = 1, predict [0.05129175 0.94870825], loss: 0.05265396086517478\n",
      "1360 iteration: y = 0, predict [0.98063392 0.01936608], loss: 0.019556061508916183\n",
      "1380 iteration: y = 0, predict [0.82208667 0.17791333], loss: 0.19590944887020054\n",
      "1400 iteration: y = 0, predict [0.9789401 0.0210599], loss: 0.02128481951714699\n",
      "1420 iteration: y = 0, predict [0.98124035 0.01875965], loss: 0.018937845538996575\n",
      "1440 iteration: y = 0, predict [0.95365985 0.04634015], loss: 0.04744821731068258\n",
      "1460 iteration: y = 1, predict [0.90188438 0.09811562], loss: 2.3216087151913207\n",
      "1480 iteration: y = 0, predict [0.95964938 0.04035062], loss: 0.041187292938662426\n",
      "1500 iteration: y = 0, predict [0.9916941 0.0083059], loss: 0.008340584993708176\n",
      "1520 iteration: y = 0, predict [0.76112169 0.23887831], loss: 0.27296202650392204\n",
      "1540 iteration: y = 1, predict [0.0647505 0.9352495], loss: 0.06694193651556486\n",
      "1560 iteration: y = 0, predict [0.90352798 0.09647202], loss: 0.10144820303898336\n",
      "1580 iteration: y = 0, predict [0.88610998 0.11389002], loss: 0.12091420660895882\n",
      "1600 iteration: y = 1, predict [0.47608873 0.52391127], loss: 0.6464329423009418\n",
      "1620 iteration: y = 0, predict [0.99067575 0.00932425], loss: 0.009367988324621159\n",
      "1640 iteration: y = 0, predict [0.98553785 0.01446215], loss: 0.014567749362966384\n",
      "1660 iteration: y = 0, predict [0.96136802 0.03863198], loss: 0.03939798551847504\n",
      "1680 iteration: y = 1, predict [0.11970765 0.88029235], loss: 0.1275012110131403\n",
      "1700 iteration: y = 1, predict [0.12704562 0.87295438], loss: 0.13587197656581895\n",
      "1720 iteration: y = 1, predict [0.0234613 0.9765387], loss: 0.02374089358130357\n",
      "1740 iteration: y = 0, predict [0.64782406 0.35217594], loss: 0.4341361374606518\n",
      "1760 iteration: y = 0, predict [0.9523388 0.0476612], loss: 0.048834425288164174\n",
      "1780 iteration: y = 0, predict [0.98835284 0.01164716], loss: 0.011715521362114008\n",
      "1800 iteration: y = 0, predict [0.96452689 0.03547311], loss: 0.036117570310464614\n",
      "1820 iteration: y = 0, predict [0.99336851 0.00663149], loss: 0.006653579005554932\n",
      "1840 iteration: y = 1, predict [0.32352034 0.67647966], loss: 0.39085289169367377\n",
      "1860 iteration: y = 0, predict [0.93869137 0.06130863], loss: 0.06326853397721081\n",
      "1880 iteration: y = 0, predict [0.99628287 0.00371713], loss: 0.0037240539476439015\n",
      "1900 iteration: y = 0, predict [0.9792838 0.0207162], loss: 0.020933792828539202\n",
      "1920 iteration: y = 1, predict [0.02708258 0.97291742], loss: 0.027456072310131734\n",
      "1940 iteration: y = 1, predict [0.03598283 0.96401717], loss: 0.03664617711230794\n",
      "1960 iteration: y = 1, predict [0.0177111 0.9822889], loss: 0.017869816942421096\n",
      "1980 iteration: y = 0, predict [0.99053276 0.00946724], loss: 0.009512335805505333\n",
      "2000 iteration: y = 0, predict [0.96602337 0.03397663], loss: 0.03456725348541162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 iteration: y = 0, predict [0.89509794 0.10490206], loss: 0.1108221389587132\n",
      "2040 iteration: y = 0, predict [0.52631884 0.47368116], loss: 0.6418480895726177\n",
      "2060 iteration: y = 0, predict [0.95094869 0.04905131], loss: 0.0502951721538602\n",
      "2080 iteration: y = 0, predict [0.96008307 0.03991693], loss: 0.04073546807006066\n",
      "2100 iteration: y = 0, predict [0.99623921 0.00376079], loss: 0.0037678816654201732\n",
      "2120 iteration: y = 1, predict [0.51564369 0.48435631], loss: 0.7249344664487594\n",
      "2140 iteration: y = 1, predict [0.13432502 0.86567498], loss: 0.1442457570856093\n",
      "2160 iteration: y = 0, predict [0.96214397 0.03785603], loss: 0.038591187672443086\n",
      "2180 iteration: y = 0, predict [0.99134274 0.00865726], loss: 0.008694954748045991\n",
      "2200 iteration: y = 0, predict [0.97060949 0.02939051], loss: 0.02983106524774029\n",
      "2220 iteration: y = 1, predict [0.07294174 0.92705826], loss: 0.07573887269130418\n",
      "2240 iteration: y = 0, predict [0.92963543 0.07036457], loss: 0.07296277604873591\n",
      "2260 iteration: y = 0, predict [0.93901627 0.06098373], loss: 0.06292247098245579\n",
      "2280 iteration: y = 0, predict [0.70420744 0.29579256], loss: 0.3506823012857997\n",
      "2300 iteration: y = 0, predict [0.76330597 0.23669403], loss: 0.2700963172243738\n",
      "2320 iteration: y = 0, predict [0.56336198 0.43663802], loss: 0.5738329149750767\n",
      "2340 iteration: y = 0, predict [0.9611045 0.0388955], loss: 0.039672134321306174\n",
      "2360 iteration: y = 0, predict [0.98377472 0.01622528], loss: 0.016358355221541356\n",
      "2380 iteration: y = 0, predict [0.9784382 0.0215618], loss: 0.02179765140495217\n",
      "2400 iteration: y = 0, predict [0.88656843 0.11343157], loss: 0.1203969658928471\n",
      "2420 iteration: y = 0, predict [0.99223283 0.00776717], loss: 0.007797495501960974\n",
      "2440 iteration: y = 0, predict [0.99326956 0.00673044], loss: 0.006753193119084629\n",
      "2460 iteration: y = 0, predict [0.96745544 0.03254456], loss: 0.03308591077195744\n",
      "2480 iteration: y = 0, predict [0.78760033 0.21239967], loss: 0.23876451617639163\n",
      "2500 iteration: y = 1, predict [0.50916029 0.49083971], loss: 0.7116376531131953\n",
      "2520 iteration: y = 1, predict [0.24504193 0.75495807], loss: 0.28109306339748097\n",
      "2540 iteration: y = 0, predict [0.95007613 0.04992387], loss: 0.0512131597493939\n",
      "2560 iteration: y = 0, predict [0.98673005 0.01326995], loss: 0.013358784067959064\n",
      "2580 iteration: y = 0, predict [0.99549355 0.00450645], loss: 0.004516629857813549\n",
      "2600 iteration: y = 0, predict [0.99046795 0.00953205], loss: 0.009577770482117113\n",
      "2620 iteration: y = 0, predict [0.9953726 0.0046274], loss: 0.0046381416973800315\n",
      "2640 iteration: y = 0, predict [0.99822064 0.00177936], loss: 0.0017809477409914266\n",
      "2660 iteration: y = 1, predict [0.01306298 0.98693702], loss: 0.01314905002503041\n",
      "2680 iteration: y = 0, predict [0.99630794 0.00369206], loss: 0.0036988884251004067\n",
      "2700 iteration: y = 0, predict [0.98739564 0.01260436], loss: 0.012684471776641016\n",
      "2720 iteration: y = 1, predict [0.0466539 0.9533461], loss: 0.047777271892431875\n",
      "2740 iteration: y = 1, predict [0.08264111 0.91735889], loss: 0.08625650518992455\n",
      "2760 iteration: y = 0, predict [0.98462078 0.01537922], loss: 0.015498706982920852\n",
      "2780 iteration: y = 1, predict [0.81084036 0.18915964], loss: 1.6651639746439582\n",
      "2800 iteration: y = 0, predict [0.99216158 0.00783842], loss: 0.00786930667338808\n",
      "2820 iteration: y = 1, predict [0.38166594 0.61833406], loss: 0.4807264167409291\n",
      "2840 iteration: y = 0, predict [0.17650365 0.82349635], loss: 1.7344137154581178\n",
      "2860 iteration: y = 0, predict [0.99447248 0.00552752], loss: 0.005542849172904674\n",
      "2880 iteration: y = 0, predict [0.98179111 0.01820889], loss: 0.018376710106850148\n",
      "2900 iteration: y = 0, predict [0.99660378 0.00339622], loss: 0.003402001412098981\n",
      "2920 iteration: y = 0, predict [0.98408532 0.01591468], loss: 0.016042674232300942\n",
      "2940 iteration: y = 0, predict [0.86258508 0.13741492], loss: 0.147821492292262\n",
      "2960 iteration: y = 0, predict [0.99658796 0.00341204], loss: 0.0034178706869167973\n",
      "2980 iteration: y = 0, predict [0.94010058 0.05989942], loss: 0.06176841257562192\n",
      "3000 iteration: y = 0, predict [0.99310658 0.00689342], loss: 0.0069172878330487145\n",
      "3020 iteration: y = 1, predict [0.01923834 0.98076166], loss: 0.01942580916758397\n",
      "3040 iteration: y = 0, predict [0.9957722 0.0042278], loss: 0.004236760785501116\n",
      "3060 iteration: y = 0, predict [0.72013333 0.27986667], loss: 0.32831889936493047\n",
      "3080 iteration: y = 1, predict [0.01577832 0.98422168], loss: 0.01590412179800994\n",
      "3100 iteration: y = 0, predict [0.90466115 0.09533885], loss: 0.10019482622221015\n",
      "3120 iteration: y = 0, predict [0.9959692 0.0040308], loss: 0.004038944842818132\n",
      "3140 iteration: y = 0, predict [0.17693433 0.82306567], loss: 1.7319766194068356\n",
      "3160 iteration: y = 0, predict [0.99122476 0.00877524], loss: 0.008813968340701321\n",
      "3180 iteration: y = 0, predict [0.99621475 0.00378525], loss: 0.0037924302379561503\n",
      "3200 iteration: y = 0, predict [0.81698143 0.18301857], loss: 0.2021389127906293\n",
      "3220 iteration: y = 0, predict [0.93020043 0.06979957], loss: 0.07235519905674002\n",
      "3240 iteration: y = 0, predict [0.71009366 0.28990634], loss: 0.3423584081921954\n",
      "3260 iteration: y = 0, predict [0.99295229 0.00704771], loss: 0.00707266595466828\n",
      "3280 iteration: y = 0, predict [0.4348886 0.5651114], loss: 0.8326653710922711\n",
      "3300 iteration: y = 0, predict [0.99435242 0.00564758], loss: 0.0056635831461244145\n",
      "3320 iteration: y = 0, predict [0.95480046 0.04519954], loss: 0.04625290628831532\n",
      "3340 iteration: y = 0, predict [0.97253053 0.02746947], loss: 0.027853806929699104\n",
      "3360 iteration: y = 1, predict [0.01573501 0.98426499], loss: 0.01586011477323978\n",
      "3380 iteration: y = 1, predict [0.08986971 0.91013029], loss: 0.09416751463014801\n",
      "3400 iteration: y = 1, predict [0.03033762 0.96966238], loss: 0.03080732661072046\n",
      "3420 iteration: y = 0, predict [0.96846756 0.03153244], loss: 0.03204029095476863\n",
      "3440 iteration: y = 0, predict [0.94722233 0.05277767], loss: 0.05422144524206528\n",
      "3460 iteration: y = 1, predict [0.87049058 0.12950942], loss: 2.044001690774814\n",
      "3480 iteration: y = 0, predict [0.9933559 0.0066441], loss: 0.006666270303072439\n",
      "3500 iteration: y = 0, predict [0.9860727 0.0139273], loss: 0.01402519026991031\n",
      "3520 iteration: y = 0, predict [0.89673847 0.10326153], loss: 0.10899102292535269\n",
      "3540 iteration: y = 0, predict [0.99527128 0.00472872], loss: 0.004739935146436113\n",
      "3560 iteration: y = 0, predict [0.99382114 0.00617886], loss: 0.006198024196721193\n",
      "3580 iteration: y = 0, predict [0.94624596 0.05375404], loss: 0.05525274411945438\n",
      "3600 iteration: y = 0, predict [0.73467616 0.26532384], loss: 0.30832547200593136\n",
      "3620 iteration: y = 0, predict [0.99690499 0.00309501], loss: 0.0030998143969981247\n",
      "3640 iteration: y = 1, predict [0.00430273 0.99569727], loss: 0.004312011156137491\n",
      "3660 iteration: y = 0, predict [0.99454676 0.00545324], loss: 0.0054681669680718015\n",
      "3680 iteration: y = 1, predict [0.03108887 0.96891113], loss: 0.03158238848759154\n",
      "3700 iteration: y = 0, predict [0.98982666 0.01017334], loss: 0.010225444624339174\n",
      "3720 iteration: y = 0, predict [0.89139114 0.10860886], loss: 0.11497196102371031\n",
      "3740 iteration: y = 0, predict [0.81054709 0.18945291], loss: 0.21004584195524081\n",
      "3760 iteration: y = 0, predict [0.99047736 0.00952264], loss: 0.009568268854965143\n",
      "3780 iteration: y = 0, predict [0.9937504 0.0062496], loss: 0.00626921459938013\n",
      "3800 iteration: y = 0, predict [0.82438161 0.17561839], loss: 0.19312173513822625\n",
      "3820 iteration: y = 0, predict [0.99741026 0.00258974], loss: 0.0025930980706721334\n",
      "3840 iteration: y = 0, predict [0.90726853 0.09273147], loss: 0.09731681314936265\n",
      "3860 iteration: y = 1, predict [0.51024765 0.48975235], loss: 0.7138554137904443\n",
      "3880 iteration: y = 0, predict [0.99154204 0.00845796], loss: 0.008493926709626442\n",
      "3900 iteration: y = 1, predict [0.09335537 0.90664463], loss: 0.09800471511432286\n",
      "3920 iteration: y = 1, predict [0.01577578 0.98422422], loss: 0.015901540508793357\n",
      "3940 iteration: y = 0, predict [0.53861778 0.46138222], loss: 0.6187490898684636\n",
      "3960 iteration: y = 1, predict [0.00556828 0.99443172], loss: 0.005583842602326667\n",
      "3980 iteration: y = 0, predict [0.98292066 0.01707934], loss: 0.017226874999878513\n",
      "4000 iteration: y = 0, predict [0.94384838 0.05615162], loss: 0.05778973643437407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4020 iteration: y = 0, predict [0.4557944 0.5442056], loss: 0.7857134501019909\n",
      "4040 iteration: y = 0, predict [0.99087988 0.00912012], loss: 0.009161960732981356\n",
      "4060 iteration: y = 0, predict [0.99613729 0.00386271], loss: 0.003870188511222291\n",
      "4080 iteration: y = 0, predict [0.99681937 0.00318063], loss: 0.003185694750023738\n",
      "4100 iteration: y = 0, predict [0.99583958 0.00416042], loss: 0.004169095791585784\n",
      "4120 iteration: y = 0, predict [0.96700518 0.03299482], loss: 0.03355142993091328\n",
      "4140 iteration: y = 0, predict [9.99200942e-01 7.99058316e-04], loss: 0.0007993777335611065\n",
      "4160 iteration: y = 1, predict [0.77096584 0.22903416], loss: 1.4738841165566547\n",
      "4180 iteration: y = 0, predict [0.34691058 0.65308942], loss: 1.0586882229652743\n",
      "4200 iteration: y = 0, predict [0.98221768 0.01778232], loss: 0.017942323068651043\n",
      "4220 iteration: y = 1, predict [0.14725418 0.85274582], loss: 0.15929375994575648\n",
      "4240 iteration: y = 0, predict [0.99554649 0.00445351], loss: 0.0044634558068640156\n",
      "4260 iteration: y = 0, predict [0.8517535 0.1482465], loss: 0.16045811422661013\n",
      "4280 iteration: y = 0, predict [0.9744585 0.0255415], loss: 0.025873346837767756\n",
      "4300 iteration: y = 1, predict [0.3622241 0.6377759], loss: 0.44976831576432574\n",
      "4320 iteration: y = 0, predict [0.99156926 0.00843074], loss: 0.00846647857175806\n",
      "4340 iteration: y = 0, predict [0.99286353 0.00713647], loss: 0.007162056482809476\n",
      "4360 iteration: y = 0, predict [0.99493797 0.00506203], loss: 0.005074881385542307\n",
      "4380 iteration: y = 0, predict [0.99173012 0.00826988], loss: 0.008304265610129402\n",
      "4400 iteration: y = 0, predict [0.99388963 0.00611037], loss: 0.00612911138277136\n",
      "4420 iteration: y = 0, predict [0.99684361 0.00315639], loss: 0.003161382732800076\n",
      "4440 iteration: y = 0, predict [0.99253267 0.00746733], loss: 0.007495347650359986\n",
      "4460 iteration: y = 1, predict [0.0770625 0.9229375], loss: 0.08019375561720259\n",
      "4480 iteration: y = 0, predict [0.99839218 0.00160782], loss: 0.0016091166203196295\n",
      "4500 iteration: y = 0, predict [0.99487628 0.00512372], loss: 0.005136893092105306\n",
      "4520 iteration: y = 1, predict [0.06470022 0.93529978], loss: 0.06688817572393141\n",
      "4540 iteration: y = 0, predict [0.99349201 0.00650799], loss: 0.006529263431281835\n",
      "4560 iteration: y = 1, predict [0.09541105 0.90458895], loss: 0.10027463715172912\n",
      "4580 iteration: y = 0, predict [0.99368204 0.00631796], loss: 0.006338002112744829\n",
      "4600 iteration: y = 1, predict [0.00540121 0.99459879], loss: 0.005415849927995502\n",
      "4620 iteration: y = 1, predict [0.72784368 0.27215632], loss: 1.3013786648499084\n",
      "4640 iteration: y = 1, predict [0.01065006 0.98934994], loss: 0.010707175770083184\n",
      "4660 iteration: y = 0, predict [0.97920074 0.02079926], loss: 0.02101860844806693\n",
      "4680 iteration: y = 0, predict [0.37256379 0.62743621], loss: 0.9873470000484487\n",
      "4700 iteration: y = 1, predict [0.00991777 0.99008223], loss: 0.009967279372162064\n",
      "4720 iteration: y = 0, predict [0.99428123 0.00571877], loss: 0.005735184352736992\n",
      "4740 iteration: y = 0, predict [0.88772978 0.11227022], loss: 0.11908788700554564\n",
      "4760 iteration: y = 1, predict [0.23050002 0.76949998], loss: 0.262014358011962\n",
      "4780 iteration: y = 0, predict [0.16165415 0.83834585], loss: 1.8222961109887363\n",
      "4800 iteration: y = 0, predict [0.98420967 0.01579033], loss: 0.015916329582334222\n",
      "4820 iteration: y = 0, predict [0.83708781 0.16291219], loss: 0.17782629852968246\n",
      "4840 iteration: y = 1, predict [0.4563632 0.5436368], loss: 0.6094739103081206\n",
      "4860 iteration: y = 1, predict [0.11031821 0.88968179], loss: 0.11689141707738165\n",
      "4880 iteration: y = 1, predict [0.17118205 0.82881795], loss: 0.18775474557148533\n",
      "4900 iteration: y = 0, predict [0.98753183 0.01246817], loss: 0.012546553277019687\n",
      "4920 iteration: y = 0, predict [0.99413493 0.00586507], loss: 0.005882333780421445\n",
      "4940 iteration: y = 0, predict [0.90637475 0.09362525], loss: 0.09830242644264324\n",
      "4960 iteration: y = 0, predict [0.99883018 0.00116982], loss: 0.0011705042413710414\n",
      "4980 iteration: y = 0, predict [0.80816706 0.19183294], loss: 0.21298648615970142\n",
      "5000 iteration: y = 0, predict [0.91174259 0.08825741], loss: 0.09239757223163446\n",
      "5020 iteration: y = 0, predict [0.99584319 0.00415681], loss: 0.004165471566460585\n",
      "5040 iteration: y = 1, predict [0.16878787 0.83121213], loss: 0.18487024668554122\n",
      "5060 iteration: y = 0, predict [0.96672198 0.03327802], loss: 0.03384433634115014\n",
      "5080 iteration: y = 0, predict [0.99320346 0.00679654], loss: 0.006819743075187715\n",
      "5100 iteration: y = 0, predict [0.9871704 0.0128296], loss: 0.012912605039462226\n",
      "5120 iteration: y = 0, predict [0.99013978 0.00986022], loss: 0.009909156266662337\n",
      "5140 iteration: y = 0, predict [0.96885095 0.03114905], loss: 0.03164449485627193\n",
      "5160 iteration: y = 0, predict [0.99602223 0.00397777], loss: 0.003985702538737626\n",
      "5180 iteration: y = 0, predict [0.98513701 0.01486299], loss: 0.01497454774579235\n",
      "5200 iteration: y = 0, predict [0.74856966 0.25143034], loss: 0.2895910143126614\n",
      "epoch: 1\n",
      "0 iteration: y = 0, predict [0.99354217 0.00645783], loss: 0.0064787692651047885\n",
      "20 iteration: y = 0, predict [0.84885502 0.15114498], loss: 0.16386686915245258\n",
      "40 iteration: y = 1, predict [0.00562699 0.99437301], loss: 0.005642880879598903\n",
      "60 iteration: y = 0, predict [9.99046234e-01 9.53766203e-04], loss: 0.0009542213275955391\n",
      "80 iteration: y = 0, predict [0.99804815 0.00195185], loss: 0.001953760452672045\n",
      "100 iteration: y = 0, predict [0.99542595 0.00457405], loss: 0.0045845479888790355\n",
      "120 iteration: y = 0, predict [0.99162733 0.00837267], loss: 0.008407913978189756\n",
      "140 iteration: y = 0, predict [0.54558339 0.45441661], loss: 0.6058996195912235\n",
      "160 iteration: y = 0, predict [0.96980967 0.03019033], loss: 0.03065544669225107\n",
      "180 iteration: y = 0, predict [0.68312064 0.31687936], loss: 0.3810838066403575\n",
      "200 iteration: y = 1, predict [0.64563505 0.35436495], loss: 1.0374279782414246\n",
      "220 iteration: y = 0, predict [0.9882191 0.0117809], loss: 0.01185084901392185\n",
      "240 iteration: y = 0, predict [0.98585348 0.01414652], loss: 0.014247534630871014\n",
      "260 iteration: y = 1, predict [0.36505731 0.63494269], loss: 0.454220540288326\n",
      "280 iteration: y = 0, predict [0.83328914 0.16671086], loss: 0.18237458629992265\n",
      "300 iteration: y = 1, predict [0.21671935 0.78328065], loss: 0.24426421176426416\n",
      "320 iteration: y = 1, predict [0.21023413 0.78976587], loss: 0.2360187412836682\n",
      "340 iteration: y = 0, predict [0.99210292 0.00789708], loss: 0.007928424071464197\n",
      "360 iteration: y = 0, predict [0.98363997 0.01636003], loss: 0.01649533193918536\n",
      "380 iteration: y = 1, predict [0.10993974 0.89006026], loss: 0.11646610969660433\n",
      "400 iteration: y = 1, predict [0.189825 0.810175], loss: 0.21050500251203239\n",
      "420 iteration: y = 1, predict [0.23890008 0.76109992], loss: 0.2729906339593517\n",
      "440 iteration: y = 0, predict [0.55301857 0.44698143], loss: 0.5923637062666927\n",
      "460 iteration: y = 0, predict [0.99858566 0.00141434], loss: 0.00141534490963719\n",
      "480 iteration: y = 1, predict [0.03425674 0.96574326], loss: 0.034857258139556424\n",
      "500 iteration: y = 1, predict [0.21120174 0.78879826], loss: 0.23724468557717956\n",
      "520 iteration: y = 1, predict [0.45923749 0.54076251], loss: 0.614775072877591\n",
      "540 iteration: y = 0, predict [0.98519572 0.01480428], loss: 0.014914961352988497\n",
      "560 iteration: y = 0, predict [0.99746211 0.00253789], loss: 0.0025411115876971036\n",
      "580 iteration: y = 0, predict [0.95701274 0.04298726], loss: 0.0439385763985573\n",
      "600 iteration: y = 1, predict [0.40087115 0.59912885], loss: 0.5122785997151642\n",
      "620 iteration: y = 0, predict [0.98196811 0.01803189], loss: 0.018196446912057813\n",
      "640 iteration: y = 0, predict [0.8516727 0.1483273], loss: 0.16055298276417826\n",
      "660 iteration: y = 0, predict [0.60772156 0.39227844], loss: 0.49803846688308473\n",
      "680 iteration: y = 0, predict [0.98479101 0.01520899], loss: 0.015325837218476317\n",
      "700 iteration: y = 0, predict [0.69282364 0.30717636], loss: 0.3669797944207974\n",
      "720 iteration: y = 1, predict [0.24044917 0.75955083], loss: 0.2750280324275547\n",
      "740 iteration: y = 0, predict [0.93792184 0.06207816], loss: 0.0640886582409428\n",
      "760 iteration: y = 0, predict [0.99116455 0.00883545], loss: 0.008874709446580491\n",
      "780 iteration: y = 0, predict [0.98431046 0.01568954], loss: 0.015813920482491944\n",
      "800 iteration: y = 0, predict [0.98460796 0.01539204], loss: 0.015511723730512865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820 iteration: y = 0, predict [0.99164193 0.00835807], loss: 0.008393194738423551\n",
      "840 iteration: y = 1, predict [0.00441583 0.99558417], loss: 0.004425607221036919\n",
      "860 iteration: y = 0, predict [0.99639564 0.00360436], loss: 0.003610873810275487\n",
      "880 iteration: y = 0, predict [0.97963992 0.02036008], loss: 0.020570203735849706\n",
      "900 iteration: y = 0, predict [0.96617488 0.03382512], loss: 0.034410424716105024\n",
      "920 iteration: y = 0, predict [0.99556311 0.00443689], loss: 0.004446757415193607\n",
      "940 iteration: y = 0, predict [0.50115606 0.49884394], loss: 0.6908377243361649\n",
      "960 iteration: y = 0, predict [0.99561851 0.00438149], loss: 0.004391114079643434\n",
      "980 iteration: y = 1, predict [0.05647711 0.94352289], loss: 0.058134653797403145\n",
      "1000 iteration: y = 0, predict [0.99670423 0.00329577], loss: 0.003301213149734862\n",
      "1020 iteration: y = 0, predict [0.98147817 0.01852183], loss: 0.018695503255189382\n",
      "1040 iteration: y = 0, predict [0.8866464 0.1133536], loss: 0.12030902834448946\n",
      "1060 iteration: y = 1, predict [0.00578263 0.99421737], loss: 0.005799409257035085\n",
      "1080 iteration: y = 0, predict [0.98429781 0.01570219], loss: 0.015826772279692005\n",
      "1100 iteration: y = 1, predict [0.02588755 0.97411245], loss: 0.026228535182211025\n",
      "1120 iteration: y = 0, predict [0.99544769 0.00455231], loss: 0.004562699296026214\n",
      "1140 iteration: y = 0, predict [0.99700193 0.00299807], loss: 0.003002571252262909\n",
      "1160 iteration: y = 0, predict [0.95155361 0.04844639], loss: 0.04965924643067757\n",
      "1180 iteration: y = 1, predict [0.02264498 0.97735502], loss: 0.022905310419012215\n",
      "1200 iteration: y = 0, predict [0.99663058 0.00336942], loss: 0.0033751059361231115\n",
      "1220 iteration: y = 0, predict [0.99029415 0.00970585], loss: 0.009753256381665538\n",
      "1240 iteration: y = 1, predict [0.97960434 0.02039566], loss: 3.8924333216961506\n",
      "1260 iteration: y = 0, predict [0.98389391 0.01610609], loss: 0.016237198064492007\n",
      "1280 iteration: y = 0, predict [0.99251336 0.00748664], loss: 0.007514803134600023\n",
      "1300 iteration: y = 0, predict [0.9627225 0.0372775], loss: 0.03799007084491739\n",
      "1320 iteration: y = 0, predict [0.84780929 0.15219071], loss: 0.16509956658073496\n",
      "1340 iteration: y = 1, predict [0.04242415 0.95757585], loss: 0.04335034047952894\n",
      "1360 iteration: y = 0, predict [0.99097818 0.00902182], loss: 0.009062761808471178\n",
      "1380 iteration: y = 0, predict [0.84145419 0.15854581], loss: 0.17262370684472758\n",
      "1400 iteration: y = 0, predict [0.99245407 0.00754593], loss: 0.0075745432371625565\n",
      "1420 iteration: y = 0, predict [0.99275038 0.00724962], loss: 0.007276031182333548\n",
      "1440 iteration: y = 0, predict [0.96318584 0.03681416], loss: 0.03750890058590956\n",
      "1460 iteration: y = 1, predict [0.88771481 0.11228519], loss: 2.186713303416726\n",
      "1480 iteration: y = 0, predict [0.97440772 0.02559228], loss: 0.025925460146364005\n",
      "1500 iteration: y = 0, predict [0.99694106 0.00305894], loss: 0.003063631821882184\n",
      "1520 iteration: y = 0, predict [0.78592446 0.21407554], loss: 0.24089460165489301\n",
      "1540 iteration: y = 1, predict [0.03944096 0.96055904], loss: 0.04023982717545104\n",
      "1560 iteration: y = 0, predict [0.91880746 0.08119254], loss: 0.08467868919697887\n",
      "1580 iteration: y = 0, predict [0.9057553 0.0942447], loss: 0.0989860951056774\n",
      "1600 iteration: y = 1, predict [0.45622079 0.54377921], loss: 0.6092119703388916\n",
      "1620 iteration: y = 0, predict [0.99648954 0.00351046], loss: 0.0035166324117016597\n",
      "1640 iteration: y = 0, predict [0.9931711 0.0068289], loss: 0.00685232281964003\n",
      "1660 iteration: y = 0, predict [0.97357969 0.02642031], loss: 0.02677559571004769\n",
      "1680 iteration: y = 1, predict [0.08227624 0.91772376], loss: 0.08585885152821426\n",
      "1700 iteration: y = 1, predict [0.08262884 0.91737116], loss: 0.0862431387059095\n",
      "1720 iteration: y = 1, predict [0.01266526 0.98733474], loss: 0.012746146760192301\n",
      "1740 iteration: y = 0, predict [0.69365545 0.30634455], loss: 0.36577991213998695\n",
      "1760 iteration: y = 0, predict [0.96374858 0.03625142], loss: 0.03692483112682591\n",
      "1780 iteration: y = 0, predict [0.99559951 0.00440049], loss: 0.004410202916675207\n",
      "1800 iteration: y = 0, predict [0.9523091 0.0476909], loss: 0.04886560903291148\n",
      "1820 iteration: y = 0, predict [0.99725663 0.00274337], loss: 0.002747143828594782\n",
      "1840 iteration: y = 1, predict [0.25687869 0.74312131], loss: 0.29689597747152996\n",
      "1860 iteration: y = 0, predict [0.94800996 0.05199004], loss: 0.053390271653481584\n",
      "1880 iteration: y = 0, predict [0.99862649 0.00137351], loss: 0.0013744499393392217\n",
      "1900 iteration: y = 0, predict [0.98943737 0.01056263], loss: 0.010618813305721208\n",
      "1920 iteration: y = 1, predict [0.01898372 0.98101628], loss: 0.01916622185460717\n",
      "1940 iteration: y = 1, predict [0.02370771 0.97629229], loss: 0.02399326325824409\n",
      "1960 iteration: y = 1, predict [0.00847703 0.99152297], loss: 0.008513164889679528\n",
      "1980 iteration: y = 0, predict [0.99617894 0.00382106], loss: 0.0038283766089860635\n",
      "2000 iteration: y = 0, predict [0.96780468 0.03219532], loss: 0.032724984225354144\n",
      "2020 iteration: y = 0, predict [0.91591065 0.08408935], loss: 0.0878364647016259\n",
      "2040 iteration: y = 0, predict [0.53605812 0.46394188], loss: 0.6235126889166153\n",
      "2060 iteration: y = 0, predict [0.96000218 0.03999782], loss: 0.04081972609292809\n",
      "2080 iteration: y = 0, predict [0.96922422 0.03077578], loss: 0.03125930260627004\n",
      "2100 iteration: y = 0, predict [0.99845065 0.00154935], loss: 0.001550551922631144\n",
      "2120 iteration: y = 1, predict [0.49751888 0.50248112], loss: 0.6881972110005945\n",
      "2140 iteration: y = 1, predict [0.08174451 0.91825549], loss: 0.08527961107642171\n",
      "2160 iteration: y = 0, predict [0.97670868 0.02329132], loss: 0.02356685410327636\n",
      "2180 iteration: y = 0, predict [0.99588786 0.00411214], loss: 0.004120617568904969\n",
      "2200 iteration: y = 0, predict [0.98508104 0.01491896], loss: 0.015031368463524899\n",
      "2220 iteration: y = 1, predict [0.06815202 0.93184798], loss: 0.07058559223209944\n",
      "2240 iteration: y = 0, predict [0.95466486 0.04533514], loss: 0.04639493509082726\n",
      "2260 iteration: y = 0, predict [0.96862225 0.03137775], loss: 0.031880577422234525\n",
      "2280 iteration: y = 0, predict [0.68902585 0.31097415], loss: 0.37247648505674585\n",
      "2300 iteration: y = 0, predict [0.82776342 0.17223658], loss: 0.1890278959999116\n",
      "2320 iteration: y = 0, predict [0.58012105 0.41987895], loss: 0.5445184885810669\n",
      "2340 iteration: y = 0, predict [0.96984653 0.03015347], loss: 0.03061743818628381\n",
      "2360 iteration: y = 0, predict [0.9907014 0.0092986], loss: 0.009342098164619582\n",
      "2380 iteration: y = 0, predict [0.98036724 0.01963276], loss: 0.019828044610069987\n",
      "2400 iteration: y = 0, predict [0.9075071 0.0924929], loss: 0.0970538852006647\n",
      "2420 iteration: y = 0, predict [0.99626562 0.00373438], loss: 0.003741375075745393\n",
      "2440 iteration: y = 0, predict [0.99727656 0.00272344], loss: 0.0027271570897347453\n",
      "2460 iteration: y = 0, predict [0.97601026 0.02398974], loss: 0.024282182238593738\n",
      "2480 iteration: y = 0, predict [0.85045145 0.14954855], loss: 0.16198794721978974\n",
      "2500 iteration: y = 1, predict [0.53154049 0.46845951], loss: 0.7583055998890322\n",
      "2520 iteration: y = 1, predict [0.21157671 0.78842329], loss: 0.23772016401125973\n",
      "2540 iteration: y = 0, predict [0.96328061 0.03671939], loss: 0.037410515307216334\n",
      "2560 iteration: y = 0, predict [0.99073349 0.00926651], loss: 0.009309708197357072\n",
      "2580 iteration: y = 0, predict [0.99818506 0.00181494], loss: 0.0018165871291671038\n",
      "2600 iteration: y = 0, predict [0.99581935 0.00418065], loss: 0.0041894101390736824\n",
      "2620 iteration: y = 0, predict [0.99772989 0.00227011], loss: 0.002272693883345204\n",
      "2640 iteration: y = 0, predict [9.99333842e-01 6.66158491e-04], loss: 0.0006663804731251537\n",
      "2660 iteration: y = 1, predict [0.00653161 0.99346839], loss: 0.006553036599485499\n",
      "2680 iteration: y = 0, predict [0.99857602 0.00142398], loss: 0.0014249903589328185\n",
      "2700 iteration: y = 0, predict [0.99362196 0.00637804], loss: 0.00639846375387246\n",
      "2720 iteration: y = 1, predict [0.04513027 0.95486973], loss: 0.046180357300248345\n",
      "2740 iteration: y = 1, predict [0.07111537 0.92888463], loss: 0.07377073578151853\n",
      "2760 iteration: y = 0, predict [0.98584506 0.01415494], loss: 0.01425607277531631\n",
      "2780 iteration: y = 1, predict [0.81017419 0.18982581], loss: 1.661648428744292\n",
      "2800 iteration: y = 0, predict [0.99599827 0.00400173], loss: 0.004009755237941955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2820 iteration: y = 1, predict [0.36831695 0.63168305], loss: 0.4593675160716172\n",
      "2840 iteration: y = 0, predict [0.14211786 0.85788214], loss: 1.9510985427602625\n",
      "2860 iteration: y = 0, predict [0.99729534 0.00270466], loss: 0.0027083218748593903\n",
      "2880 iteration: y = 0, predict [0.98793955 0.01206045], loss: 0.012133768758281366\n",
      "2900 iteration: y = 0, predict [0.99862241 0.00137759], loss: 0.0013785369781036435\n",
      "2920 iteration: y = 0, predict [0.99248006 0.00751994], loss: 0.007548359303640926\n",
      "2940 iteration: y = 0, predict [0.87090887 0.12909113], loss: 0.13821793957296793\n",
      "2960 iteration: y = 0, predict [0.99835613 0.00164387], loss: 0.0016452240960042152\n",
      "2980 iteration: y = 0, predict [0.94062367 0.05937633], loss: 0.06121214133690425\n",
      "3000 iteration: y = 0, predict [0.99688194 0.00311806], loss: 0.003122926881679208\n",
      "3020 iteration: y = 1, predict [0.01444311 0.98555689], loss: 0.014548425663805382\n",
      "3040 iteration: y = 0, predict [0.99814353 0.00185647], loss: 0.0018581954371953732\n",
      "3060 iteration: y = 0, predict [0.76613559 0.23386441], loss: 0.26639611469173147\n",
      "3080 iteration: y = 1, predict [0.00922093 0.99077907], loss: 0.009263708245679908\n",
      "3100 iteration: y = 0, predict [0.93608741 0.06391259], loss: 0.06604641949251164\n",
      "3120 iteration: y = 0, predict [0.99784655 0.00215345], loss: 0.002155769680166669\n",
      "3140 iteration: y = 0, predict [0.17302738 0.82697262], loss: 1.7543054022870221\n",
      "3160 iteration: y = 0, predict [0.99524321 0.00475679], loss: 0.004768140190821788\n",
      "3180 iteration: y = 0, predict [0.99819169 0.00180831], loss: 0.0018099509772521903\n",
      "3200 iteration: y = 0, predict [0.83364412 0.16635588], loss: 0.18194867689363084\n",
      "3220 iteration: y = 0, predict [0.92205439 0.07794561], loss: 0.08115106601621946\n",
      "3240 iteration: y = 0, predict [0.73364279 0.26635721], loss: 0.3097330287680828\n",
      "3260 iteration: y = 0, predict [0.99695139 0.00304861], loss: 0.003053267314400228\n",
      "3280 iteration: y = 0, predict [0.42289868 0.57710132], loss: 0.8606226515913259\n",
      "3300 iteration: y = 0, predict [0.99705728 0.00294272], loss: 0.0029470579180687685\n",
      "3320 iteration: y = 0, predict [0.96047763 0.03952237], loss: 0.040324584949084585\n",
      "3340 iteration: y = 0, predict [0.97287762 0.02712238], loss: 0.02749698236957697\n",
      "3360 iteration: y = 1, predict [0.00824547 0.99175453], loss: 0.008279655174821227\n",
      "3380 iteration: y = 1, predict [0.06021083 0.93978917], loss: 0.06209971593008615\n",
      "3400 iteration: y = 1, predict [0.01965136 0.98034864], loss: 0.019847018905272237\n",
      "3420 iteration: y = 0, predict [0.98098167 0.01901833], loss: 0.019201505739368477\n",
      "3440 iteration: y = 0, predict [0.95545481 0.04454519], loss: 0.04556781039946026\n",
      "3460 iteration: y = 1, predict [0.84831702 0.15168298], loss: 1.8859625680025494\n",
      "3480 iteration: y = 0, predict [0.99658315 0.00341685], loss: 0.0034227008678420477\n",
      "3500 iteration: y = 0, predict [0.99033234 0.00966766], loss: 0.00971469724451977\n",
      "3520 iteration: y = 0, predict [0.90277206 0.09722794], loss: 0.10228517818410855\n",
      "3540 iteration: y = 0, predict [0.99758175 0.00241825], loss: 0.0024211825077507703\n",
      "3560 iteration: y = 0, predict [0.99626241 0.00373759], loss: 0.0037445959030703653\n",
      "3580 iteration: y = 0, predict [0.96072822 0.03927178], loss: 0.04006372296443548\n",
      "3600 iteration: y = 0, predict [0.73157314 0.26842686], loss: 0.31255807506049016\n",
      "3620 iteration: y = 0, predict [0.99844336 0.00155664], loss: 0.0015578560071769965\n",
      "3640 iteration: y = 1, predict [0.00191726 0.99808274], loss: 0.0019190973631473067\n",
      "3660 iteration: y = 0, predict [0.99694145 0.00305855], loss: 0.0030632397881725933\n",
      "3680 iteration: y = 1, predict [0.02756844 0.97243156], loss: 0.027955576499775814\n",
      "3700 iteration: y = 0, predict [0.9942395 0.0057605], loss: 0.005777154857189407\n",
      "3720 iteration: y = 0, predict [0.92614664 0.07385336], loss: 0.07672269626785568\n",
      "3740 iteration: y = 0, predict [0.86607043 0.13392957], loss: 0.14378904968300188\n",
      "3760 iteration: y = 0, predict [0.99520654 0.00479346], loss: 0.0048049889767757545\n",
      "3780 iteration: y = 0, predict [0.99650948 0.00349052], loss: 0.003496625192148379\n",
      "3800 iteration: y = 0, predict [0.81503847 0.18496153], loss: 0.20451996198032285\n",
      "3820 iteration: y = 0, predict [0.99887474 0.00112526], loss: 0.0011258912774312268\n",
      "3840 iteration: y = 0, predict [0.94445844 0.05554156], loss: 0.05714359810447363\n",
      "3860 iteration: y = 1, predict [0.45735234 0.54264766], loss: 0.611295053608764\n",
      "3880 iteration: y = 0, predict [0.99475804 0.00524196], loss: 0.005255748899223827\n",
      "3900 iteration: y = 1, predict [0.04158348 0.95841652], loss: 0.04247281394563154\n",
      "3920 iteration: y = 1, predict [0.00858378 0.99141622], loss: 0.008620837482163265\n",
      "3940 iteration: y = 0, predict [0.43478605 0.56521395], loss: 0.8329012043729177\n",
      "3960 iteration: y = 1, predict [0.00294685 0.99705315], loss: 0.0029512053414242055\n",
      "3980 iteration: y = 0, predict [0.98492031 0.01507969], loss: 0.015194543779320235\n",
      "4000 iteration: y = 0, predict [0.96382303 0.03617697], loss: 0.036847580269587246\n",
      "4020 iteration: y = 0, predict [0.48451686 0.51548314], loss: 0.724603056749511\n",
      "4040 iteration: y = 0, predict [0.99291241 0.00708759], loss: 0.007112825588211121\n",
      "4060 iteration: y = 0, predict [0.99805213 0.00194787], loss: 0.0019497709631746342\n",
      "4080 iteration: y = 0, predict [0.99832219 0.00167781], loss: 0.001679215026466878\n",
      "4100 iteration: y = 0, predict [0.99760208 0.00239792], loss: 0.0024007980265239293\n",
      "4120 iteration: y = 0, predict [0.96905944 0.03094056], loss: 0.031429332353319184\n",
      "4140 iteration: y = 0, predict [9.99632777e-01 3.67223022e-04], loss: 0.0003672904651129284\n",
      "4160 iteration: y = 1, predict [0.75829672 0.24170328], loss: 1.4200444071550955\n",
      "4180 iteration: y = 0, predict [0.38470602 0.61529398], loss: 0.9552758312190289\n",
      "4200 iteration: y = 0, predict [0.98709919 0.01290081], loss: 0.01298474530097286\n",
      "4220 iteration: y = 1, predict [0.13331834 0.86668166], loss: 0.1430835385732475\n",
      "4240 iteration: y = 0, predict [0.99748995 0.00251005], loss: 0.0025132096500269444\n",
      "4260 iteration: y = 0, predict [0.8909131 0.1090869], loss: 0.11550838309710715\n",
      "4280 iteration: y = 0, predict [0.98195366 0.01804634], loss: 0.018211161015088875\n",
      "4300 iteration: y = 1, predict [0.3836311 0.6163689], loss: 0.48390963108910445\n",
      "4320 iteration: y = 0, predict [0.99489561 0.00510439], loss: 0.00511746559944466\n",
      "4340 iteration: y = 0, predict [0.99547004 0.00452996], loss: 0.0045402493426507165\n",
      "4360 iteration: y = 0, predict [0.99722643 0.00277357], loss: 0.002777422270612828\n",
      "4380 iteration: y = 0, predict [0.9939501 0.0060499], loss: 0.006068273686587341\n",
      "4400 iteration: y = 0, predict [0.99644059 0.00355941], loss: 0.0035657585742618686\n",
      "4420 iteration: y = 0, predict [0.99795627 0.00204373], loss: 0.002045819522881515\n",
      "4440 iteration: y = 0, predict [0.99561377 0.00438623], loss: 0.004395875219672981\n",
      "4460 iteration: y = 1, predict [0.05643716 0.94356284], loss: 0.05809231392158335\n",
      "4480 iteration: y = 0, predict [9.99236535e-01 7.63464755e-04], loss: 0.0007637563426637602\n",
      "4500 iteration: y = 0, predict [0.99727443 0.00272557], loss: 0.002729291072190899\n",
      "4520 iteration: y = 1, predict [0.03879396 0.96120604], loss: 0.03956649236799455\n",
      "4540 iteration: y = 0, predict [0.99639197 0.00360803], loss: 0.0036145591893768135\n",
      "4560 iteration: y = 1, predict [0.08585738 0.91414262], loss: 0.08976867490252223\n",
      "4580 iteration: y = 0, predict [0.99646115 0.00353885], loss: 0.003545122260429224\n",
      "4600 iteration: y = 1, predict [0.00314465 0.99685535], loss: 0.0031496070274709153\n",
      "4620 iteration: y = 1, predict [0.71919191 0.28080809], loss: 1.2700837930603754\n",
      "4640 iteration: y = 1, predict [0.00599598 0.99400402], loss: 0.00601402937912412\n",
      "4660 iteration: y = 0, predict [0.98356482 0.01643518], loss: 0.01657173569132418\n",
      "4680 iteration: y = 0, predict [0.36529379 0.63470621], loss: 1.0070533562499044\n",
      "4700 iteration: y = 1, predict [0.00657846 0.99342154], loss: 0.006600193451240714\n",
      "4720 iteration: y = 0, predict [0.99656929 0.00343071], loss: 0.00343660901574822\n",
      "4740 iteration: y = 0, predict [0.91642431 0.08357569], loss: 0.0872758052452739\n",
      "4760 iteration: y = 1, predict [0.23162321 0.76837679], loss: 0.26347505838175495\n",
      "4780 iteration: y = 0, predict [0.13782265 0.86217735], loss: 1.9817875848919775\n",
      "4800 iteration: y = 0, predict [0.98862491 0.01137509], loss: 0.011440280592735242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4820 iteration: y = 0, predict [0.84391047 0.15608953], loss: 0.16970886883777728\n",
      "4840 iteration: y = 1, predict [0.42484963 0.57515037], loss: 0.5531237677023118\n",
      "4860 iteration: y = 1, predict [0.08940892 0.91059108], loss: 0.09366135341047015\n",
      "4880 iteration: y = 1, predict [0.14087793 0.85912207], loss: 0.15184425946070457\n",
      "4900 iteration: y = 0, predict [0.98942916 0.01057084], loss: 0.010627107976186217\n",
      "4920 iteration: y = 0, predict [0.9965996 0.0034004], loss: 0.0034061902994252245\n",
      "4940 iteration: y = 0, predict [0.92049031 0.07950969], loss: 0.08284880702472437\n",
      "4960 iteration: y = 0, predict [9.99356583e-01 6.43417319e-04], loss: 0.000643624401109682\n",
      "4980 iteration: y = 0, predict [0.84605991 0.15394009], loss: 0.1671651105210601\n",
      "5000 iteration: y = 0, predict [0.94353195 0.05646805], loss: 0.05812505353805612\n",
      "5020 iteration: y = 0, predict [0.99746387 0.00253613], loss: 0.0025393470510960163\n",
      "5040 iteration: y = 1, predict [0.15166269 0.84833731], loss: 0.1644769543708264\n",
      "5060 iteration: y = 0, predict [0.9741827 0.0258173], loss: 0.02615641136610493\n",
      "5080 iteration: y = 0, predict [0.99443688 0.00556312], loss: 0.00557864822276817\n",
      "5100 iteration: y = 0, predict [0.99013526 0.00986474], loss: 0.009913714378358855\n",
      "5120 iteration: y = 0, predict [0.98992406 0.01007594], loss: 0.010127049563132427\n",
      "5140 iteration: y = 0, predict [0.98285223 0.01714777], loss: 0.017296493563159775\n",
      "5160 iteration: y = 0, predict [0.99776357 0.00223643], loss: 0.0022389378488155256\n",
      "5180 iteration: y = 0, predict [0.98886404 0.01113596], loss: 0.011198431949397937\n",
      "5200 iteration: y = 0, predict [0.7450097 0.2549903], loss: 0.29435803599631566\n",
      "epoch: 2\n",
      "0 iteration: y = 0, predict [0.99573152 0.00426848], loss: 0.004277617909622249\n",
      "20 iteration: y = 0, predict [0.89268996 0.10731004], loss: 0.11351594757597502\n",
      "40 iteration: y = 1, predict [0.00315833 0.99684167], loss: 0.0031633262554440996\n",
      "60 iteration: y = 0, predict [9.99487299e-01 5.12701477e-04], loss: 0.0005128329528740228\n",
      "80 iteration: y = 0, predict [0.99887981 0.00112019], loss: 0.0011208193063084082\n",
      "100 iteration: y = 0, predict [0.99736613 0.00263387], loss: 0.0026373400724775966\n",
      "120 iteration: y = 0, predict [0.99423135 0.00576865], loss: 0.005785353054370319\n",
      "140 iteration: y = 0, predict [0.56096473 0.43903527], loss: 0.5780972389028677\n",
      "160 iteration: y = 0, predict [0.96791106 0.03208894], loss: 0.032615076089526095\n",
      "180 iteration: y = 0, predict [0.68731284 0.31268716], loss: 0.3749657231098584\n",
      "200 iteration: y = 1, predict [0.62105167 0.37894833], loss: 0.9703554250094631\n",
      "220 iteration: y = 0, predict [0.99135192 0.00864808], loss: 0.008685694117361652\n",
      "240 iteration: y = 0, predict [0.98911426 0.01088574], loss: 0.010945424117268506\n",
      "260 iteration: y = 1, predict [0.30397086 0.69602914], loss: 0.362363757466099\n",
      "280 iteration: y = 0, predict [0.86492495 0.13507505], loss: 0.14511253933678356\n",
      "300 iteration: y = 1, predict [0.20770196 0.79229804], loss: 0.23281764432199945\n",
      "320 iteration: y = 1, predict [0.19634847 0.80365153], loss: 0.21858952525991424\n",
      "340 iteration: y = 0, predict [0.99446326 0.00553674], loss: 0.005552124963556687\n",
      "360 iteration: y = 0, predict [0.99157948 0.00842052], loss: 0.008456171738135785\n",
      "380 iteration: y = 1, predict [0.08860335 0.91139665], loss: 0.09277707954263706\n",
      "400 iteration: y = 1, predict [0.13132694 0.86867306], loss: 0.14078845288942693\n",
      "420 iteration: y = 1, predict [0.2221177 0.7778823], loss: 0.2511800516633483\n",
      "440 iteration: y = 0, predict [0.68623234 0.31376766], loss: 0.3765390147270781\n",
      "460 iteration: y = 0, predict [9.99243913e-01 7.56087022e-04], loss: 0.0007563729995411297\n",
      "480 iteration: y = 1, predict [0.02369266 0.97630734], loss: 0.023977844494398323\n",
      "500 iteration: y = 1, predict [0.17335422 0.82664578], loss: 0.1903789918310446\n",
      "520 iteration: y = 1, predict [0.39068997 0.60931003], loss: 0.4954280664883148\n",
      "540 iteration: y = 0, predict [0.99108781 0.00891219], loss: 0.008952141294771771\n",
      "560 iteration: y = 0, predict [0.99853358 0.00146642], loss: 0.0014674950223593983\n",
      "580 iteration: y = 0, predict [0.9658247 0.0341753], loss: 0.03477293594863487\n",
      "600 iteration: y = 1, predict [0.31835989 0.68164011], loss: 0.38325346321776366\n",
      "620 iteration: y = 0, predict [0.98729904 0.01270096], loss: 0.012782304177134468\n",
      "640 iteration: y = 0, predict [0.8962234 0.1037766], loss: 0.10956557036540414\n",
      "660 iteration: y = 0, predict [0.63106351 0.36893649], loss: 0.4603487793726661\n",
      "680 iteration: y = 0, predict [0.98715004 0.01284996], loss: 0.01293323537908515\n",
      "700 iteration: y = 0, predict [0.70475493 0.29524507], loss: 0.3499051492049874\n",
      "720 iteration: y = 1, predict [0.22116587 0.77883413], loss: 0.24995718446891407\n",
      "740 iteration: y = 0, predict [0.95650191 0.04349809], loss: 0.0444724940778169\n",
      "760 iteration: y = 0, predict [0.99511537 0.00488463], loss: 0.004896602627202722\n",
      "780 iteration: y = 0, predict [0.99043522 0.00956478], loss: 0.009610817821116188\n",
      "800 iteration: y = 0, predict [0.98551389 0.01448611], loss: 0.014592058890132376\n",
      "820 iteration: y = 0, predict [0.9940506 0.0059494], loss: 0.005967172149055994\n",
      "840 iteration: y = 1, predict [0.00259492 0.99740508], loss: 0.0025982886909526903\n",
      "860 iteration: y = 0, predict [0.99794682 0.00205318], loss: 0.0020552950595438954\n",
      "880 iteration: y = 0, predict [0.98668705 0.01331295], loss: 0.013402363951570018\n",
      "900 iteration: y = 0, predict [0.97341812 0.02658188], loss: 0.02694156336007601\n",
      "920 iteration: y = 0, predict [0.99738798 0.00261202], loss: 0.0026154339754432153\n",
      "940 iteration: y = 0, predict [0.49172093 0.50827907], loss: 0.7098439340237443\n",
      "960 iteration: y = 0, predict [0.99737356 0.00262644], loss: 0.0026298985314174065\n",
      "980 iteration: y = 1, predict [0.04650754 0.95349246], loss: 0.04762376286866324\n",
      "1000 iteration: y = 0, predict [0.99799329 0.00200671], loss: 0.002008721645324408\n",
      "1020 iteration: y = 0, predict [0.98519886 0.01480114], loss: 0.01491176543780037\n",
      "1040 iteration: y = 0, predict [0.91968626 0.08031374], loss: 0.08372269340006327\n",
      "1060 iteration: y = 1, predict [0.00335278 0.99664722], loss: 0.0033584096363236155\n",
      "1080 iteration: y = 0, predict [0.98723045 0.01276955], loss: 0.012851786177782853\n",
      "1100 iteration: y = 1, predict [0.01841029 0.98158971], loss: 0.018581868818927365\n",
      "1120 iteration: y = 0, predict [0.99745056 0.00254944], loss: 0.0025526999742733476\n",
      "1140 iteration: y = 0, predict [0.99824221 0.00175779], loss: 0.0017593414630728266\n",
      "1160 iteration: y = 0, predict [0.94872955 0.05127045], loss: 0.052631508236187084\n",
      "1180 iteration: y = 1, predict [0.01774747 0.98225253], loss: 0.017906842848916114\n",
      "1200 iteration: y = 0, predict [0.9978415 0.0021585], loss: 0.0021608358102263403\n",
      "1220 iteration: y = 0, predict [0.99436739 0.00563261], loss: 0.005648536152106494\n",
      "1240 iteration: y = 1, predict [0.9810614 0.0189386], loss: 3.966552911173099\n",
      "1260 iteration: y = 0, predict [0.98710447 0.01289553], loss: 0.012979396238917525\n",
      "1280 iteration: y = 0, predict [0.9943867 0.0056133], loss: 0.0056291186904043776\n",
      "1300 iteration: y = 0, predict [0.9690311 0.0309689], loss: 0.03145857668557298\n",
      "1320 iteration: y = 0, predict [0.8561383 0.1438617], loss: 0.15532335601253702\n",
      "1340 iteration: y = 1, predict [0.03832456 0.96167544], loss: 0.039078266946044715\n",
      "1360 iteration: y = 0, predict [0.99278671 0.00721329], loss: 0.007239426939847903\n",
      "1380 iteration: y = 0, predict [0.86432957 0.13567043], loss: 0.14580113608696127\n",
      "1400 iteration: y = 0, predict [0.99518255 0.00481745], loss: 0.004829092805836744\n",
      "1420 iteration: y = 0, predict [0.99537592 0.00462408], loss: 0.004634808516982778\n",
      "1440 iteration: y = 0, predict [0.96998937 0.03001063], loss: 0.030470171243756512\n",
      "1460 iteration: y = 1, predict [0.88222661 0.11777339], loss: 2.1389929166892814\n",
      "1480 iteration: y = 0, predict [0.98038548 0.01961452], loss: 0.01980944122075835\n",
      "1500 iteration: y = 0, predict [0.99804535 0.00195465], loss: 0.0019565579052869234\n",
      "1520 iteration: y = 0, predict [0.82022252 0.17977748], loss: 0.19817960633358458\n",
      "1540 iteration: y = 1, predict [0.02569381 0.97430619], loss: 0.026029657969025944\n",
      "1560 iteration: y = 0, predict [0.92874702 0.07125298], loss: 0.07391888630375526\n",
      "1580 iteration: y = 0, predict [0.9216595 0.0783405], loss: 0.08157942599203791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 iteration: y = 1, predict [0.43104553 0.56895447], loss: 0.5639548685292936\n",
      "1620 iteration: y = 0, predict [0.99779597 0.00220403], loss: 0.0022064580766978513\n",
      "1640 iteration: y = 0, predict [0.99397117 0.00602883], loss: 0.006047076672901498\n",
      "1660 iteration: y = 0, predict [0.97846286 0.02153714], loss: 0.02177244455162333\n",
      "1680 iteration: y = 1, predict [0.05995127 0.94004873], loss: 0.06182356461567838\n",
      "1700 iteration: y = 1, predict [0.05624558 0.94375442], loss: 0.0578892944497554\n",
      "1720 iteration: y = 1, predict [0.00796001 0.99203999], loss: 0.007991857624073537\n",
      "1740 iteration: y = 0, predict [0.74389902 0.25610098], loss: 0.29584997442073585\n",
      "1760 iteration: y = 0, predict [0.97110953 0.02889047], loss: 0.029316010980447112\n",
      "1780 iteration: y = 0, predict [0.99719246 0.00280754], loss: 0.0028114888717674247\n",
      "1800 iteration: y = 0, predict [0.94445398 0.05554602], loss: 0.057148314825517355\n",
      "1820 iteration: y = 0, predict [0.99821358 0.00178642], loss: 0.001788014312812933\n",
      "1840 iteration: y = 1, predict [0.18892792 0.81107208], loss: 0.20939834692784587\n",
      "1860 iteration: y = 0, predict [0.95601387 0.04398613], loss: 0.044982854434517404\n",
      "1880 iteration: y = 0, predict [9.99139161e-01 8.60838754e-04], loss: 0.0008612094885489195\n",
      "1900 iteration: y = 0, predict [0.99127359 0.00872641], loss: 0.008764709867141665\n",
      "1920 iteration: y = 1, predict [0.01521661 0.98478339], loss: 0.015333568480075731\n",
      "1940 iteration: y = 1, predict [0.01765337 0.98234663], loss: 0.0178110499020027\n",
      "1960 iteration: y = 1, predict [0.00557164 0.99442836], loss: 0.005587222162802272\n",
      "1980 iteration: y = 0, predict [0.99755123 0.00244877], loss: 0.0024517712241553173\n",
      "2000 iteration: y = 0, predict [0.96710453 0.03289547], loss: 0.03344869453274242\n",
      "2020 iteration: y = 0, predict [0.92949345 0.07050655], loss: 0.07311551595807239\n",
      "2040 iteration: y = 0, predict [0.55478437 0.44521563], loss: 0.5891757623174082\n",
      "2060 iteration: y = 0, predict [0.96697358 0.03302642], loss: 0.033584108105057235\n",
      "2080 iteration: y = 0, predict [0.9744199 0.0255801], loss: 0.025912961152121244\n",
      "2100 iteration: y = 0, predict [9.99016833e-01 9.83166579e-04], loss: 0.0009836502046628804\n",
      "2120 iteration: y = 1, predict [0.47124214 0.52875786], loss: 0.6372246895026535\n",
      "2140 iteration: y = 1, predict [0.05716435 0.94283565], loss: 0.05886329558428837\n",
      "2160 iteration: y = 0, predict [0.98347628 0.01652372], loss: 0.01666175974114085\n",
      "2180 iteration: y = 0, predict [0.9972477 0.0027523], loss: 0.0027560927830138637\n",
      "2200 iteration: y = 0, predict [0.99114981 0.00885019], loss: 0.008889589485723298\n",
      "2220 iteration: y = 1, predict [0.06590994 0.93409006], loss: 0.0681824252962512\n",
      "2240 iteration: y = 0, predict [0.96922246 0.03077754], loss: 0.031261115729309175\n",
      "2260 iteration: y = 0, predict [0.98282508 0.01717492], loss: 0.017324120307780903\n",
      "2280 iteration: y = 0, predict [0.66596128 0.33403872], loss: 0.4065237453082575\n",
      "2300 iteration: y = 0, predict [0.87165123 0.12834877], loss: 0.1373659011380189\n",
      "2320 iteration: y = 0, predict [0.58378768 0.41621232], loss: 0.5382179233744092\n",
      "2340 iteration: y = 0, predict [0.97245247 0.02754753], loss: 0.02793407694842873\n",
      "2360 iteration: y = 0, predict [0.9930053 0.0069947], loss: 0.007019281402846592\n",
      "2380 iteration: y = 0, predict [0.98148906 0.01851094], loss: 0.018684412184734572\n",
      "2400 iteration: y = 0, predict [0.91443576 0.08556424], loss: 0.08944805614531956\n",
      "2420 iteration: y = 0, predict [0.99746931 0.00253069], loss: 0.0025338938390302703\n",
      "2440 iteration: y = 0, predict [0.99841849 0.00158151], loss: 0.001582761895921549\n",
      "2460 iteration: y = 0, predict [0.9812223 0.0187777], loss: 0.018956243934324626\n",
      "2480 iteration: y = 0, predict [0.89169989 0.10830011], loss: 0.1146256451442103\n",
      "2500 iteration: y = 1, predict [0.53580178 0.46419822], loss: 0.7674436227255527\n",
      "2520 iteration: y = 1, predict [0.18179188 0.81820812], loss: 0.20063855309456508\n",
      "2540 iteration: y = 0, predict [0.97100268 0.02899732], loss: 0.029426054227659162\n",
      "2560 iteration: y = 0, predict [0.99232434 0.00767566], loss: 0.007705273425481042\n",
      "2580 iteration: y = 0, predict [0.99894784 0.00105216], loss: 0.0010527117043226055\n",
      "2600 iteration: y = 0, predict [0.99742089 0.00257911], loss: 0.0025824412097546624\n",
      "2620 iteration: y = 0, predict [0.99842375 0.00157625], loss: 0.0015774965970133184\n",
      "2640 iteration: y = 0, predict [9.99631569e-01 3.68430715e-04], loss: 0.0003684986021632665\n",
      "2660 iteration: y = 1, predict [0.00473063 0.99526937], loss: 0.004741853916249376\n",
      "2680 iteration: y = 0, predict [9.99194326e-01 8.05674375e-04], loss: 0.0008059991045385648\n",
      "2700 iteration: y = 0, predict [0.99520276 0.00479724], loss: 0.0048087873310464066\n",
      "2720 iteration: y = 1, predict [0.04362059 0.95637941], loss: 0.04460057382823429\n",
      "2740 iteration: y = 1, predict [0.06166937 0.93833063], loss: 0.06365291235824942\n",
      "2760 iteration: y = 0, predict [0.98706632 0.01293368], loss: 0.013018045346953673\n",
      "2780 iteration: y = 1, predict [0.8121853 0.1878147], loss: 1.6722994255351895\n",
      "2800 iteration: y = 0, predict [0.99723688 0.00276312], loss: 0.002766941312247979\n",
      "2820 iteration: y = 1, predict [0.35910209 0.64089791], loss: 0.44488509833943407\n",
      "2840 iteration: y = 0, predict [0.12004443 0.87995557], loss: 2.119893351042743\n",
      "2860 iteration: y = 0, predict [0.99819752 0.00180248], loss: 0.0018041114162999992\n",
      "2880 iteration: y = 0, predict [0.98916725 0.01083275], loss: 0.010891849123592687\n",
      "2900 iteration: y = 0, predict [9.99200621e-01 7.99378607e-04], loss: 0.0007996982808288347\n",
      "2920 iteration: y = 0, predict [0.99565584 0.00434416], loss: 0.0043536209656544\n",
      "2940 iteration: y = 0, predict [0.88290737 0.11709263], loss: 0.12453499317936406\n",
      "2960 iteration: y = 0, predict [0.99890174 0.00109826], loss: 0.0010988655466695628\n",
      "2980 iteration: y = 0, predict [0.94228353 0.05771647], loss: 0.059449058182339296\n",
      "3000 iteration: y = 0, predict [0.9980532 0.0019468], loss: 0.001948701227238037\n",
      "3020 iteration: y = 1, predict [0.01160271 0.98839729], loss: 0.011670551534503582\n",
      "3040 iteration: y = 0, predict [0.99889936 0.00110064], loss: 0.001101248800318532\n",
      "3060 iteration: y = 0, predict [0.80565723 0.19434277], loss: 0.21609690141470292\n",
      "3080 iteration: y = 1, predict [0.0061977 0.9938023], loss: 0.006216990212351615\n",
      "3100 iteration: y = 0, predict [0.95417376 0.04582624], loss: 0.046909488669975\n",
      "3120 iteration: y = 0, predict [0.9984993 0.0015007], loss: 0.0015018281337712842\n",
      "3140 iteration: y = 0, predict [0.17149078 0.82850922], loss: 1.7632257566139937\n",
      "3160 iteration: y = 0, predict [0.99641753 0.00358247], loss: 0.003588905107477123\n",
      "3180 iteration: y = 0, predict [0.99885353 0.00114647], loss: 0.0011471324578688304\n",
      "3200 iteration: y = 0, predict [0.83456253 0.16543747], loss: 0.18084761229383342\n",
      "3220 iteration: y = 0, predict [0.9187549 0.0812451], loss: 0.08473589110328208\n",
      "3240 iteration: y = 0, predict [0.7592623 0.2407377], loss: 0.27540797326914745\n",
      "3260 iteration: y = 0, predict [0.99823502 0.00176498], loss: 0.0017665394286558696\n",
      "3280 iteration: y = 0, predict [0.4077699 0.5922301], loss: 0.8970522374792096\n",
      "3300 iteration: y = 0, predict [0.99806904 0.00193096], loss: 0.0019328218604671634\n",
      "3320 iteration: y = 0, predict [0.96490227 0.03509773], loss: 0.035728454074186566\n",
      "3340 iteration: y = 0, predict [0.97303165 0.02696835], loss: 0.02733866477242553\n",
      "3360 iteration: y = 1, predict [0.00463149 0.99536851], loss: 0.00464224548967958\n",
      "3380 iteration: y = 1, predict [0.04027758 0.95972242], loss: 0.04111118158796718\n",
      "3400 iteration: y = 1, predict [0.01423604 0.98576396], loss: 0.014338347569504683\n",
      "3420 iteration: y = 0, predict [0.98673706 0.01326294], loss: 0.013351679356783405\n",
      "3440 iteration: y = 0, predict [0.96081298 0.03918702], loss: 0.039975502908700236\n",
      "3460 iteration: y = 1, predict [0.81996772 0.18003228], loss: 1.714619090858729\n",
      "3480 iteration: y = 0, predict [0.99774056 0.00225944], loss: 0.002261998494780362\n",
      "3500 iteration: y = 0, predict [0.99092543 0.00907457], loss: 0.009115992970677084\n",
      "3520 iteration: y = 0, predict [0.90615344 0.09384656], loss: 0.09854662201228939\n",
      "3540 iteration: y = 0, predict [0.99838055 0.00161945], loss: 0.0016207631894047943\n",
      "3560 iteration: y = 0, predict [0.9971283 0.0028717], loss: 0.002875829565595029\n",
      "3580 iteration: y = 0, predict [0.96811719 0.03188281], loss: 0.03240213660205087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600 iteration: y = 0, predict [0.73192646 0.26807354], loss: 0.3120752383615586\n",
      "3620 iteration: y = 0, predict [0.99897446 0.00102554], loss: 0.0010260691163657407\n",
      "3640 iteration: y = 1, predict [0.00115946 0.99884054], loss: 0.001160133653380289\n",
      "3660 iteration: y = 0, predict [0.99782741 0.00217259], loss: 0.002174953998020044\n",
      "3680 iteration: y = 1, predict [0.02581254 0.97418746], loss: 0.026151532211373632\n",
      "3700 iteration: y = 0, predict [0.99590106 0.00409894], loss: 0.00410736309961111\n",
      "3720 iteration: y = 0, predict [0.94975021 0.05024979], loss: 0.051556268534013946\n",
      "3740 iteration: y = 0, predict [0.9014447 0.0985553], loss: 0.1037565837481649\n",
      "3760 iteration: y = 0, predict [0.99682782 0.00317218], loss: 0.0031772189095153214\n",
      "3780 iteration: y = 0, predict [0.99740415 0.00259585], loss: 0.0025992256850154182\n",
      "3800 iteration: y = 0, predict [0.81877489 0.18122511], loss: 0.19994609749582515\n",
      "3820 iteration: y = 0, predict [9.99351525e-01 6.48475100e-04], loss: 0.000648685451384896\n",
      "3840 iteration: y = 0, predict [0.96286644 0.03713356], loss: 0.03784056911713125\n",
      "3860 iteration: y = 1, predict [0.40084974 0.59915026], loss: 0.5122428540696784\n",
      "3880 iteration: y = 0, predict [0.99597455 0.00402545], loss: 0.00403357671344051\n",
      "3900 iteration: y = 1, predict [0.02354649 0.97645351], loss: 0.023828139658747954\n",
      "3920 iteration: y = 1, predict [0.00554388 0.99445612], loss: 0.0055593067903372964\n",
      "3940 iteration: y = 0, predict [0.37995106 0.62004894], loss: 0.9677128262241751\n",
      "3960 iteration: y = 1, predict [0.00196642 0.99803358], loss: 0.0019683593929397595\n",
      "3980 iteration: y = 0, predict [0.98615498 0.01384502], loss: 0.013941760029308364\n",
      "4000 iteration: y = 0, predict [0.97402931 0.02597069], loss: 0.026313885001446432\n",
      "4020 iteration: y = 0, predict [0.52532735 0.47467265], loss: 0.6437336829893998\n",
      "4040 iteration: y = 0, predict [0.9942735 0.0057265], loss: 0.005742961757512374\n",
      "4060 iteration: y = 0, predict [0.99851108 0.00148892], loss: 0.0014900300631599224\n",
      "4080 iteration: y = 0, predict [0.99888469 0.00111531], loss: 0.001115937238219418\n",
      "4100 iteration: y = 0, predict [0.99835725 0.00164275], loss: 0.0016441013454402745\n",
      "4120 iteration: y = 0, predict [0.96915196 0.03084804], loss: 0.03133385850099831\n",
      "4140 iteration: y = 0, predict [9.99790318e-01 2.09682296e-04], loss: 0.0002097042829049351\n",
      "4160 iteration: y = 1, predict [0.74893137 0.25106863], loss: 1.3820289542222695\n",
      "4180 iteration: y = 0, predict [0.43228757 0.56771243], loss: 0.8386642339573432\n",
      "4200 iteration: y = 0, predict [0.98977983 0.01022017], loss: 0.01027275581386859\n",
      "4220 iteration: y = 1, predict [0.11854242 0.88145758], loss: 0.12617839924180244\n",
      "4240 iteration: y = 0, predict [0.99830841 0.00169159], loss: 0.0016930224033661879\n",
      "4260 iteration: y = 0, predict [0.915393 0.084607], loss: 0.08840179255564891\n",
      "4280 iteration: y = 0, predict [0.98635211 0.01364789], loss: 0.013741882834508367\n",
      "4300 iteration: y = 1, predict [0.3949137 0.6050863], loss: 0.5023841892881503\n",
      "4320 iteration: y = 0, predict [0.99642091 0.00357909], loss: 0.003585510648998683\n",
      "4340 iteration: y = 0, predict [0.99674998 0.00325002], loss: 0.00325531682914283\n",
      "4360 iteration: y = 0, predict [0.998245 0.001755], loss: 0.00175653911657356\n",
      "4380 iteration: y = 0, predict [0.99495413 0.00504587], loss: 0.005058643464511401\n",
      "4400 iteration: y = 0, predict [0.9975723 0.0024277], loss: 0.0024306531227569947\n",
      "4420 iteration: y = 0, predict [0.99848787 0.00151213], loss: 0.001513270103007477\n",
      "4440 iteration: y = 0, predict [0.9970208 0.0029792], loss: 0.002983648493404385\n",
      "4460 iteration: y = 1, predict [0.04481291 0.95518709], loss: 0.04584804923547256\n",
      "4480 iteration: y = 0, predict [9.99559793e-01 4.40206577e-04], loss: 0.0004403034966555916\n",
      "4500 iteration: y = 0, predict [0.99828736 0.00171264], loss: 0.0017141099746935493\n",
      "4520 iteration: y = 1, predict [0.0267337 0.9732663], loss: 0.02709754804223171\n",
      "4540 iteration: y = 0, predict [0.99770348 0.00229652], loss: 0.002299156116295132\n",
      "4560 iteration: y = 1, predict [0.0796721 0.9203279], loss: 0.08302525611554656\n",
      "4580 iteration: y = 0, predict [0.99768695 0.00231305], loss: 0.0023157320232271355\n",
      "4600 iteration: y = 1, predict [0.00209551 0.99790449], loss: 0.0020977072483210997\n",
      "4620 iteration: y = 1, predict [0.68709822 0.31290178], loss: 1.1618659344433162\n",
      "4640 iteration: y = 1, predict [0.00388567 0.99611433], loss: 0.003893236422882382\n",
      "4660 iteration: y = 0, predict [0.98652229 0.01347771], loss: 0.01356936271068893\n",
      "4680 iteration: y = 0, predict [0.34855861 0.65144139], loss: 1.0539488906850911\n",
      "4700 iteration: y = 1, predict [0.00506373 0.99493627], loss: 0.005076589760240347\n",
      "4720 iteration: y = 0, predict [0.99766218 0.00233782], loss: 0.002340560865060591\n",
      "4740 iteration: y = 0, predict [0.93912801 0.06087199], loss: 0.06280348187794678\n",
      "4760 iteration: y = 1, predict [0.24263017 0.75736983], loss: 0.2779036037655653\n",
      "4780 iteration: y = 0, predict [0.11837097 0.88162903], loss: 2.1339318048689604\n",
      "4800 iteration: y = 0, predict [0.99009138 0.00990862], loss: 0.00995803926036866\n",
      "4820 iteration: y = 0, predict [0.85060064 0.14939936], loss: 0.1618125446894725\n",
      "4840 iteration: y = 1, predict [0.39679413 0.60320587], loss: 0.5054967308890314\n",
      "4860 iteration: y = 1, predict [0.07418153 0.92581847], loss: 0.077077095021973\n",
      "4880 iteration: y = 1, predict [0.11904516 0.88095484], loss: 0.12674891496448795\n",
      "4900 iteration: y = 0, predict [0.99047016 0.00952984], loss: 0.009575540670896726\n",
      "4920 iteration: y = 0, predict [0.99776676 0.00223324], loss: 0.0022357375502952935\n",
      "4940 iteration: y = 0, predict [0.92763339 0.07236661], loss: 0.07511867341878876\n",
      "4960 iteration: y = 0, predict [9.99584599e-01 4.15400535e-04], loss: 0.0004154868374367753\n",
      "4980 iteration: y = 0, predict [0.86853375 0.13146625], loss: 0.14094883686176887\n",
      "5000 iteration: y = 0, predict [0.96167898 0.03832102], loss: 0.039074589372542705\n",
      "5020 iteration: y = 0, predict [0.99825716 0.00174284], loss: 0.0017443583101913105\n",
      "5040 iteration: y = 1, predict [0.14369581 0.85630419], loss: 0.1551296040437428\n",
      "5060 iteration: y = 0, predict [0.97866074 0.02133926], loss: 0.021570231070495702\n",
      "5080 iteration: y = 0, predict [0.99527604 0.00472396], loss: 0.004735155665029411\n",
      "5100 iteration: y = 0, predict [0.99103828 0.00896172], loss: 0.009002115547410447\n",
      "5120 iteration: y = 0, predict [0.98953607 0.01046393], loss: 0.01051905939431084\n",
      "5140 iteration: y = 0, predict [0.98946514 0.01053486], loss: 0.01059074389950962\n",
      "5160 iteration: y = 0, predict [0.99855338 0.00144662], loss: 0.0014476721090343299\n",
      "5180 iteration: y = 0, predict [0.99087148 0.00912852], loss: 0.009170440320761534\n",
      "5200 iteration: y = 0, predict [0.72763365 0.27236635], loss: 0.3179575884849733\n",
      "epoch: 3\n",
      "0 iteration: y = 0, predict [0.99679294 0.00320706], loss: 0.0032122157323255\n",
      "20 iteration: y = 0, predict [0.9212841 0.0787159], loss: 0.08198681826276843\n",
      "40 iteration: y = 1, predict [0.00210112 0.99789888], loss: 0.0021033312463451348\n",
      "60 iteration: y = 0, predict [9.99681589e-01 3.18410996e-04], loss: 0.00031846169920985405\n",
      "80 iteration: y = 0, predict [9.9927869e-01 7.2131027e-04], loss: 0.0007215705389916276\n",
      "100 iteration: y = 0, predict [0.99826695 0.00173305], loss: 0.0017345554601663235\n",
      "120 iteration: y = 0, predict [0.99562616 0.00437384], loss: 0.004383433294568784\n",
      "140 iteration: y = 0, predict [0.58293642 0.41706358], loss: 0.5396771543757389\n",
      "160 iteration: y = 0, predict [0.96445479 0.03554521], loss: 0.03619232287408821\n",
      "180 iteration: y = 0, predict [0.69756528 0.30243472], loss: 0.3601591773757848\n",
      "200 iteration: y = 1, predict [0.59050567 0.40949433], loss: 0.8928322167756402\n",
      "220 iteration: y = 0, predict [0.99319779 0.00680221], loss: 0.0068254529697879185\n",
      "240 iteration: y = 0, predict [0.99130402 0.00869598], loss: 0.00873401154030618\n",
      "260 iteration: y = 1, predict [0.2535067 0.7464933], loss: 0.2923686319610985\n",
      "280 iteration: y = 0, predict [0.89787133 0.10212867], loss: 0.1077285032231626\n",
      "300 iteration: y = 1, predict [0.20473727 0.79526273], loss: 0.22908273820226538\n",
      "320 iteration: y = 1, predict [0.18717579 0.81282421], loss: 0.2072404183490688\n",
      "340 iteration: y = 0, predict [0.99582245 0.00417755], loss: 0.004186296836883163\n",
      "360 iteration: y = 0, predict [0.99516744 0.00483256], loss: 0.004844270465269145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380 iteration: y = 1, predict [0.07837547 0.92162453], loss: 0.0816173737780705\n",
      "400 iteration: y = 1, predict [0.09630657 0.90369343], loss: 0.10126509830274712\n",
      "420 iteration: y = 1, predict [0.20214959 0.79785041], loss: 0.22583415495686446\n",
      "440 iteration: y = 0, predict [0.7813548 0.2186452], loss: 0.24672594000351003\n",
      "460 iteration: y = 0, predict [9.99542009e-01 4.57991290e-04], loss: 0.000458096200113075\n",
      "Training is early stopped!\n"
     ]
    }
   ],
   "source": [
    "model_1, E_xs = train(model_1, X_train, y_train, learning_rate=0.0001, epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06bb09b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8028846153846154\n"
     ]
    }
   ],
   "source": [
    "metrics(model_1, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e70768f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16126,), numpy.ndarray)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_xs.shape, type(E_xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "082c6ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_max(l):\n",
    "    max_idx = np.argmax(l)\n",
    "    max_val = l[max_idx]\n",
    "    return (max_idx, max_val)\n",
    "\n",
    "def metrics(model, X_test, y_test):\n",
    "    correct = 0\n",
    "    total = len(y_test)\n",
    "    for l in range(len(X_test)):\n",
    "        X_ = X_test[l]\n",
    "        y_ = y_test[l]\n",
    "        t = [1 if i == y_ else 0 for i in [0, 1]]\n",
    "        i = t.index(1)\n",
    "        z = X_\n",
    "        for layer in model:\n",
    "            W, b = layer.copy()\n",
    "            a = np.dot(W, z) + b\n",
    "            z = relu(a)\n",
    "        y = softmax(z)\n",
    "        \n",
    "        ind, val = np_max(y)\n",
    "#         print(y_, \":\", y[i], f\"predict for {labels[ind]}:\",  val)\n",
    "        if ind == i:\n",
    "            correct += 1\n",
    "    print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a1a7d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = np.array([])\n",
    "step = 100\n",
    "for i in range(step, len(E_xs), step):\n",
    "    loss = np.append(loss, E_xs[i-step:i].min())\n",
    "print(loss.size)\n",
    "plt.plot(np.arange(len(loss)), loss)\n",
    "plt.title(\"Loss function\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.savefig(\"mlp_lear_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "d8cd6bb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_point(model_1, 1, E_xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7de3a2ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_eary_stopped\n",
      "epoch_eary_stopped/error\n",
      "epoch_eary_stopped/layer_0\n",
      "epoch_eary_stopped/layer_0/bias\n",
      "epoch_eary_stopped/layer_0/weight\n",
      "epoch_eary_stopped/layer_1\n",
      "epoch_eary_stopped/layer_1/bias\n",
      "epoch_eary_stopped/layer_1/weight\n",
      "epoch_eary_stopped/layer_2\n",
      "epoch_eary_stopped/layer_2/bias\n",
      "epoch_eary_stopped/layer_2/weight\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File('mlp_model_for_xray_ds_learned.hdf5', 'r')\n",
    "def printname(name):\n",
    "    print(name)\n",
    "\n",
    "f.visit(printname)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20f70254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['epoch_eary_stopped']>\n",
      "['error', 'layer_0', 'layer_1', 'layer_2']\n",
      "(100, 22500)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('mlp_model_for_xray_ds_learned.hdf5', 'r') as f:\n",
    "    print(f.keys())\n",
    "    epoch = f['epoch_eary_stopped']\n",
    "    grp_l = epoch[f\"layer_{0}\"]\n",
    "    print(list(epoch.keys()))\n",
    "    test_weight = grp_l['weight']\n",
    "    print(test_weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df4f56ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d04b3f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "if f: print(\"access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f1f18e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3199146/1609336080.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.array(model_1).shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(model_1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5dbdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = check_point_return(model_1, 1) # epoch or 'early_stopped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78432f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
